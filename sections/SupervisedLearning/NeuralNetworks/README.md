# Neural Networks

A neural network can be considered a "universal function approximator". If you're an absolute math god you can think of these as similar to a [Taylor Series](https://en.wikipedia.org/wiki/Taylor_series) - essentially a way to take a function that already exists
and represent it in a different way. The main difference is that Neural Networks can approximate functions that we don't already have
standardized mathematical equations for!

The essence of Neural networks is learning how to associate a set of features (input) with a desired label (output). They create this association via training algorithms. The type of training we will be learning here is gradient discent via back propogation.

# A Simple Diagram

Feature - \\
Feature -  \\
             ---- A Neural Network ----> Label
Feature -  /
Feature - /

We can break training down into two parts:
* Forward Pass
* Back propogation

# Forward Pass

